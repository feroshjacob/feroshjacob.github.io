
<div class="panel panel-default">
  <div class="panel-heading">Presentations</div>
  <div class="panel-body">
		  <p class="centerimage">
	<strong>Full-time software engineer and Part-time Instructor</strong>
<br/>
Most of the students enrolled in a computer science course are looking for a career in a software company. As a software engineer myself, I can help them understand the concepts from an application perspective. 
In this session, I will talk about my experience of teaching CS 1301, an introductory course on a popular programming language (JAVA).  
The course was labelled as a gateway course with high DFIW rate. For most of the students, this was their first programming language first experience and many were scared from the beginning whether they can pass the course.
The talk will include the details of some of the strategies I applied to get students attention, keep them focused in the topic, 
improve their confidence, get curious and excited about programming in general. My strategies were mostly based on my 15 years of experience as a programmer. 

<br/>
<i>  <a href="files/space.pptx">SPACE conference, Kennesaw, GA, June 2019</a></i>
<br/>  
	  <p class="centerimage">
	<strong>Deep Learning with Python: from Theory to Application</strong>
<br/>
This tutorial gave a comprehensive overview of the deep learning. The goal was to make deep learning accessible to engineers who seek to apply deep learning to problems they are trying to solve both in industry and academia. 
<br/>
<i>  <a href="https://github.com/feroshjacob/dl-using-tensorflow-acmse2019-tutorial/blob/master/ACMSE-v1-session2.pptx">Session 2, ACMSE, Kennesaw, GA, March 2019/a></i>
<br/>
		  
<p class="centerimage">
	<strong>Ontology-based semantic search</strong>
<br/>
Two key factors of any document search are : 1) How much we understand the input queries and 2) How organized is the data in which we are searching. In this talk, I show how an ontology can be leveraged for 
understanding and organizing data. To create and maintain an ontology can be cumbersome, I describe some of the challenges I faced, lesson learned, and assumptions made while developing an ontology for the home improvement/retail domain. 
<br/>
<i>  <a href="http://ccse.kennesaw.edu/acmse/keynote-speakers.php">ACMSE, Kennesaw, GA 2017</a> <a href="files/waltonhighschool.pptx">Walton High School, Georgia March 2019</a></i>
<br/>

<p class="centerimage">
	<strong>Machine learning techniques in Java</strong>
<br/>
In the field of artificial intelligence, there are several reliable open-source tools and libraries implemented in Java. At The Home Depot, 
we have many projects that are implemented in Java that make use of state-of-the-art machine learning techniques. In this talk, we give a brief overview 
of some of these projects along with their motivation and overall impact. The talk also includes a short demo on how to implement a machine learning 
based solution for a real world application in the home improvement/ retail domain. 
<br/>
<i>Slides</i>: <a href="files/ajug2016.pptx">Atlanta Java Users Group (AJUG) 2016</a> 
<br/>

<p class="centerimage">
	<strong>WebScalding: A Framework for Big Data Web Services</strong>
<br/>
CareerBuilder (CB) currently has 50 million active resumes and 2 million active job postings. Our team has been working to provide the most relevant jobs
 for job seekers and resumes for employers and recruiters. These goals often lead to Big Data problems. In this paper, we introduce WebScalding, a Big Data 
framework designed and developed to solve some of the common large scale data challenges at CB. The WebScalding framework raises the level of abstraction of Twitter's Scalding framework to adapt to CB's unique challenges. The WebScalding framework helps users by ensuring that: 1) All internal web services are available as 
cascading pipe operations, 2) These pipe operations can read from our common data sources and create a pipe assembly and, 3) The pipe assembly such 
created can be executed in the CB Hadoop cluster as well as local machines without making any changes. We describe WebScalding using three case studies 
taken from actual internal projects that explain how data scientists at CB not well versed in Big Data tools and methodologies leverage WebScalding to 
design, implement, and test Big Data applications. We also compare the execution time of a WebScalding program with its sequential Python counterpart to 
illustrate the super linear speed up of WebScalding programs.
<br/>
<i>Slides</i>: <a href="files/bigdataservice2015.pptx">BigDataService 2015</a> 
<br/>

<p class="centerimage">
	<strong>sCooL- Academic Entity Normalization</strong>
<br/>

Named Entity Normalization involves normalizing recognized entities to a concrete, unambiguous real world entity. Within the purview of the online job posting domain,
academic institution name normalization provides a beneficial opportunity for CareerBuilder (CB).
Accurate and detailed normalization of academic institutions are important  to perform  sophisticated labor market dynamics analysis.    
In this paper we present and discuss the design and the implementation of sCooL, an academic institution name normalization system designed to supplant the 
existing manually maintained mapping system at CB. We also discuss the specific challenges that led  to the design of sCooL. 
sCooL leverages Wikipedia to create academic institution name mappings from a school database which is created from job applicant resumes posted on our website.
The mappings created are utilized to build a database which is then used for normalization. sCooL  provides the flexibility to integrate mappings collected from 
different curated and non-curated sources. The system is able to identify malformed data and K-12 schools from universities and colleges. We conduct an extensive 
comparative evaluation of the semi-automated sCooL system against the existing manual mapping implementation and show that sCooL provides better coverage with improved accuracy. 

<br/>
<i>Slides</i>: <a href="files/bddac_2014.pptx">BDDAC 2014</a> 
<br/>
</p>

<p class="centerimage">
	<strong>Domain-specific languages for composing signature discovery workflows</strong>
<br/>

Domain-agnostic signature discovery entails study across multiple scientific disciplines. The cross-disciplinary nature and breadth of this work requires that existing executable applications be integrated with new capabilities into workflows, representing a wide range of user tasks. An algorithm may be written in multiple programming languages for various hardware platforms, and so workflow composition requires integrating executables from any number of remote hosts. This raises
an engineering issue on how to generate web service wrappers for these heterogeneous executables and to compose them into a scientific workflow environment (e.g., Taverna). In this position paper, we summarize our work on two simple Domain-Specific Languages (DSLs) that automate these processes. Our Service Description Language (SDL) describes key elements of a signature discovery service and automatically generates its implementation code. The Workflow Description Language (WDL) describes the
pipeline of services and generates deployable artifacts for the Taverna workflow management system. We demonstrate our approach with a real-world workflow composed of services wrapping remote executables.
<br/>
<i>Slides</i>: <a href="files/dsm_2012.pptx">Splash DSM 2012</a> 
<br/>
</p>

	<p class="centerimage">
	<strong>Modeling code sections</strong>
<br/>
Graphical Processing Unit (GPU) programming languages are used extensively for general-purpose computations. However, GPU programming may not be the solution for a given problem and a given architecture. This gives rise to the need for having an abstract way to express the parallel problems. This poster presents a new approach through which programmers can access these languages without having to focus on the technical or language-specific details. An earlier approach of Abstract
Application Programming Interface (API) targeted for GPU programming is extended to shared memory using OpenMP.

<br/>
<i>Slides</i>: <a href="files/splash2010src.pdf">Splash 2010 SRC</a>, <a href="files/acmse-2011-slides.pdf">ACMSE 2011</a>
<br/>
</p>
	<p class="centerimage">
	<strong>GPU programming</strong>
<br/>
Graphical Processing Unit (GPU) programming languages are used extensively for general-purpose computations. However, GPU programming languages are at a level of abstraction suitable only for use by expert parallel programmers. This paper presents a new approach through which 'C' or Java programmers can access these languages without having to focus on the technical or language-specific details. A prototype of the approach, named CUDACL, is introduced through which a programmer can
specify one or more parallel blocks in a file and execute in a graphical processing unit. CUDACL also helps the programmer to make CUDA or OpenCL kernel calls inside an existing program. Two scenarios have been successfully implemented to assess the usability and potential of the tool. The tool was created based on a detailed anlysis of the CUDA and OpenCL programs. Our evaluation of CUDACL compared to other similar approaches shows the efficiency and effectiveness of CUDACL.
<br/>
<i>Slides</i>: <a href="files/hipc2010.pdf">HIPC 2010</a>, <a href="files/hipc2010srs.pdf">HIPC SRS 2010</a>  
<br/>
Programming GPUs has several challenges with respect to the amount of efforts spent in combining the kernel functional code of an application with the parallel concerns offered by APIs from various GPUs. This project introduces our approach for raising the level of abstaction for programming GPUs. We have implemented an abstract API that can be used with the Compute Unified Device Architecture (CUDA) and the Open Compute Language (OpenCL) frameworks, so that the mechanical steps involved in
writing the GPU code are abstracted in separate modules. The approach involves static code analysis and generative programming techniques for automatically generating the host code required for CUDA and OpenCL frameworks from minimal specifications provided by the programmers. The generated code resembles the hand-written code and the performance of the generated code is comparable to the hand-written code.
<br/>
<i>Slides</i>: <a href="files/pdpta2010.pdf">PDPTA 2010</a>  
</p>
	<p class="centerimage">
	<strong>Code templates</strong>
<br/>
This project investigates the use of a natural language processing technique that automatically detects project-specific code templates (i.e., frequently used code blocks), which can be made available to software developers within an integrated development environment. During software development, programmers often and in some cases unknowingly rewrite the same code block that represents some functionality. These frequently used code blocks can inform the existence and possible use of code
templates. Many existing code editors support code templates, but programmers are expected to manually define these templates and subsequently add them as templates in the editor. Furthermore, the support of editors to provide templates based on the editing context is still limited. The use of n-gram language models within the context of software development is described and evaluated to overcome these restrictions. The technique can search for project-specific code templates and
present these templates to the programmer based on the current editing context. 
<br/>
<i>Slides</i>: <a href="files/acmse2010.pdf">ACMSE 2010</a> 
<br/>
</p>

	<p class="centerimage">
	<strong>CSeR</strong>
<br/>
Tool support for code clones can improve software quality and maintainability. While significant research has been done in locating clones in existing source code, there has been less of a research focus on pro-actively tracking and supporting copy-paste modify operations, even though copying and pasting is a major source of clone formation and the resulting clones are then often modified. We designed and implemented a programming editor, based on the Eclipse integrated development
environment, named CSeR (Code Segment Reuse), which keeps a record of copy-and paste-induced clones and then tracks and visualizes the changes made to a clone with distinct colors. The core of CSeR is an algorithm that actively compares two clones for detailed differences as a programmer edits either one of them. This edit-based comparison algorithm is unique to CSeR and produces more immediate, accurate, and natural results than other differencing tools. 
<br/>
<i>Slides</i>: <a href="files/iwsc2010.pdf">ICSE IWSC 2010</a> 
<br/>
</p>
</div>
</div>
